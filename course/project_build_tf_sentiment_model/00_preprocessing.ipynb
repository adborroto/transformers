{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "We will be using the [Rotten Tomatoes movie reviews dataset](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.tsv.zip to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.28M/1.28M [00:00<00:00, 6.49MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading test.tsv.zip to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 494k/494k [00:00<00:00, 4.53MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "for file in ['train.tsv', 'test.tsv']:\n",
    "    api.competition_download_file('sentiment-analysis-on-movie-reviews', f'{file}.zip', path='./')\n",
    "\n",
    "    with zipfile.ZipFile(f'{file}.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "\n",
    "    os.remove(f'{file}.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparing Data\n",
    "\n",
    "We will start by reading the data into a Pandas Dataframe using th `read_csv` function. Because we're working with *.tsv* (*tab seperate values*) files we need to specify that we will be taking tab characters as the delimiters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Phrase* column contains all of our text data that we will be processing. We can also see that there are many copies through *segments* of the same answer (note that the *SentenceId* value for each of these copies is identical). We can reduce the amount of noise in our dataset by removing these duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df.drop_duplicates(subset=['SentenceId'], keep='first')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of sentiment classes across our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be tokenizing this text to create two input tensors; our input IDs, and attention mask.\n",
    "\n",
    "We will contain our tensors within two numpy arrays, which will be of dimensions `len(df) * 512` - the `512` is the sequence length of our tokenized sequences for BERT, and `len(df)` the number of samples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_len = 512\n",
    "num_samples = len(df)\n",
    "\n",
    "num_samples, seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin tokenizing with a `BertTokenizer`, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be52ef3bd3664ed3b7e3f995db12b67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3142f4d4e66e43cabef59e24f38d8a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7251eb80099d4328982b9cafa261ecf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87ba68d045c4d6486047aec8a7976ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# tokenize - this time returning Numpy tensors\n",
    "tokens = tokenizer(df['Phrase'].tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which returns us three numpy arrays - *input_ids*, *token_type_ids*, and *attention_mask*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,   138,  1326, ...,     0,     0,     0],\n",
       "       [  101,   138,  1326, ...,     0,     0,     0],\n",
       "       [  101,   138,  1326, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 13936, 25265, ...,     0,     0,     0],\n",
       "       [  101, 13936, 25265, ...,     0,     0,     0],\n",
       "       [  101, 15107,  1103, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['attention_mask'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we save them to file as Numpy binary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie-xids.npy', 'wb') as f:\n",
    "    np.save(f, tokens['input_ids'])\n",
    "with open('movie-xmask.npy', 'wb') as f:\n",
    "    np.save(f, tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have them on file, we can delete the in-memory arrays to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input tensors are prepared, but we haven't touched our target data yet. So, let's move onto that.\n",
    "\n",
    "Presently our target data is a set of integer values (representing sentiment classes) in the *Sentiment* column of our dataframe `df`. We need to extract these values and *one-hot* encode them into another numpy array, which will have the dimensions `len(df) * number of label classes`. Again, we will initialize a numpy zero array beforehand, but we won't populate it row by row - we will use some fancy indexing techniques instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first extract sentiment column\n",
    "arr = df['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we then initialize the zero array\n",
    "labels = np.zeros((num_samples, arr.max()+1))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to use `arr.max()+1` to define our second dimension here because we have the values *\\[0, 1, 2, 3, 4\\]* in our *Sentiment* column, there are **five** unique labels which means we need our labels array to have five columns (one for each) - `arr.max() = 4`, so we do `4 + 1` to get our required value of `5`.\n",
    "\n",
    "Now we use the current values in our `arr` of *\\[0, 1, 2, 3, 4\\]* to place `1` values in the correct positions of our presently zeros-only array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.arange(num_samples), arr] = 1\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is our one-hot encoded labels array. Just like before, we save this to file as a Numpy binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie-labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
