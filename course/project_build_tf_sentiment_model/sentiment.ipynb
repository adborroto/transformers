{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  A series of escapades demonstrating the adage ...          1\n",
       "1  A series of escapades demonstrating the adage ...          2\n",
       "2                                           A series          2\n",
       "3                                                  A          2\n",
       "4                                             series          2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Phrase', 'Sentiment']].head(1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=512,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids = np.zeros((len(df), 512))\n",
    "Xmask = np.zeros((len(df), 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sequence in enumerate(df['Phrase']):\n",
    "    tokens = tokenize(sequence)\n",
    "    Xids[i, :], Xmask[i, :] = tokens[0], tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,\n",
       "       3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 3, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 2,\n",
       "       4, 3, 2, 3, 3, 3, 2, 2, 4, 2, 3, 4, 2, 2, 2, 1, 2, 2, 2, 3, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 0, 2, 0, 2, 1, 1, 1, 2, 2,\n",
       "       1, 2, 2, 2, 2, 2, 3, 4, 4, 3, 3, 3, 3, 4, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       2, 3, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 3, 3, 3, 1,\n",
       "       2, 2, 1, 0, 2, 0, 1, 2, 1, 1, 2, 2, 4, 3, 2, 2, 3, 2, 4, 2, 3, 2,\n",
       "       4, 3, 3, 3, 4, 2, 4, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       1, 2, 1, 0, 2, 1, 2, 2, 2, 1, 0, 1, 0, 1, 1, 3, 2, 3, 2, 3, 2, 2,\n",
       "       3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 0, 2, 1,\n",
       "       0, 0, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 4, 3, 2, 2, 2, 1,\n",
       "       2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 3, 2, 2, 3, 4, 3, 3, 1, 2,\n",
       "       2, 3, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       4, 2, 2, 4, 3, 2, 4, 2, 3, 2, 4, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2,\n",
       "       2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 3, 1, 1, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 4, 2, 2, 1, 3, 3, 3, 3, 2, 2, 1, 0, 2, 2, 2, 1, 1, 2, 1, 2, 1,\n",
       "       2, 2, 2, 3, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 4, 2, 3, 3, 3, 3, 3, 3,\n",
       "       2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 4, 2, 3, 3,\n",
       "       2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 4, 3, 4, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 3, 2, 3, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       2, 4, 4, 3, 4, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1,\n",
       "       2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2,\n",
       "       3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 2, 2, 2, 3, 3, 4, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 3, 2, 3, 3, 2, 2,\n",
       "       2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 2, 2, 2, 2,\n",
       "       2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 1,\n",
       "       1, 2, 2, 3, 2, 3, 2, 4, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 1,\n",
       "       2, 3, 2, 2, 2, 3, 2, 2, 1, 1, 2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1,\n",
       "       1, 2, 1, 0, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 3, 2,\n",
       "       2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2,\n",
       "       2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 1,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 3, 2, 2, 2,\n",
       "       2, 2, 2, 2, 3, 2, 4, 4, 3, 2, 3, 1, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2,\n",
       "       3, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 3, 1, 2, 2, 2, 2, 1, 1,\n",
       "       1, 1, 1, 2, 2, 3, 2, 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros((arr.size, arr.max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(arr.size), arr] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie-xids.npy', 'wb') as f:\n",
    "    np.save(f, Xids)\n",
    "with open('movie-xmask.npy', 'wb') as f:\n",
    "    np.save(f, Xmask)\n",
    "with open('movie-labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, Xids, Xmask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie-xids.npy', 'rb') as f:\n",
    "    Xids = np.load(f, allow_pickle=True)\n",
    "with open('movie-xmask.npy', 'rb') as f:\n",
    "    Xmask = np.load(f, allow_pickle=True)\n",
    "with open('movie-labels.npy', 'rb') as f:\n",
    "    labels = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)  # required to avoid GPU LSTM Internal Error\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0-dev20201110'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))  # [750000:850000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE = 100000\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.shuffle(SHUFFLE).batch(BATCH_SIZE) #, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIZE = Xids.shape[0]/BATCH_SIZE\n",
    "SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 0.9\n",
    "\n",
    "train = data.take(int(SIZE*SPLIT))\n",
    "val = data.skip(int(SIZE*SPLIT))\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-cased')  #, output_hidden_states=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:4955: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[0]  # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.1)(embeddings)\n",
    "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 512, 768)     0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 768)          0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 5)            3845        global_max_pooling1d[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 108,314,117\n",
      "Trainable params: 3,845\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "56/56 [==============================] - 23s 252ms/step - loss: 2.3173 - accuracy: 0.5451 - val_loss: 1.1069 - val_accuracy: 0.7308\n",
      "Epoch 2/2\n",
      "56/56 [==============================] - 11s 205ms/step - loss: 1.3653 - accuracy: 0.6339 - val_loss: 0.9378 - val_accuracy: 0.6635\n"
     ]
    }
   ],
   "source": [
    "# 800K\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1395: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 512, 768)     0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 768)          0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 5)            3845        global_max_pooling1d[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 108,314,117\n",
      "Trainable params: 108,314,117\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('sentiment_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 139ms/step - loss: 1.0678 - accuracy: 0.6923\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.61600770e-03, 1.61399581e-02, 8.81537974e-01, 9.45726037e-02,\n",
       "        1.33427122e-04],\n",
       "       [7.02638784e-03, 2.99034566e-01, 5.73699236e-01, 1.20218106e-01,\n",
       "        2.17534525e-05],\n",
       "       [2.13267319e-02, 2.18926340e-01, 6.04214907e-01, 1.55486241e-01,\n",
       "        4.58343056e-05],\n",
       "       [6.09720917e-03, 5.33939958e-01, 1.81329250e-01, 2.78631181e-01,\n",
       "        2.43848808e-06],\n",
       "       [4.22458537e-03, 2.40804236e-02, 9.37473118e-01, 3.41925099e-02,\n",
       "        2.94180991e-05],\n",
       "       [4.67320643e-02, 1.65817395e-01, 2.94325203e-01, 4.93072838e-01,\n",
       "        5.25151663e-05],\n",
       "       [2.95730587e-02, 1.96691960e-01, 6.20805681e-01, 1.52830005e-01,\n",
       "        9.93432404e-05],\n",
       "       [2.04604883e-02, 9.57593694e-02, 5.34271896e-01, 3.49503189e-01,\n",
       "        5.10916652e-06],\n",
       "       [1.22610498e-02, 5.12559175e-01, 9.00671557e-02, 3.85110855e-01,\n",
       "        1.68314602e-06],\n",
       "       [1.17646297e-02, 7.94910938e-02, 7.08183110e-01, 2.00488031e-01,\n",
       "        7.31378386e-05],\n",
       "       [3.86042823e-03, 1.81455724e-02, 9.18010592e-01, 5.98562397e-02,\n",
       "        1.27112522e-04],\n",
       "       [6.06805459e-03, 3.73511314e-02, 9.07838166e-01, 4.86939959e-02,\n",
       "        4.87398247e-05],\n",
       "       [3.58931674e-03, 4.07180451e-02, 9.33486044e-01, 2.22013574e-02,\n",
       "        5.20235699e-06],\n",
       "       [6.37175655e-03, 2.03274652e-01, 7.10924506e-01, 7.94014037e-02,\n",
       "        2.76621940e-05],\n",
       "       [5.28250113e-02, 1.53791696e-01, 2.68768102e-01, 5.24287462e-01,\n",
       "        3.27764050e-04],\n",
       "       [3.04988539e-03, 3.75805683e-02, 8.31508994e-01, 1.27729237e-01,\n",
       "        1.31284512e-04]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(val.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (None, 512), attention_mask: (None, 512)}, (None, 5)), types: ({input_ids: tf.float64, attention_mask: tf.float64}, tf.float64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
