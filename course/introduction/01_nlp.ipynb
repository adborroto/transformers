{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Neural Nets\n",
    "\n",
    "NLP really blew up with the 2013 paper introducing word2vec by Mikolov et al \\[1\\]. This introduced a way to represent similarity and relationships between words through the use of word vectors.\n",
    "\n",
    "![The words 'hello' and 'world' in vector format](../../assets/images/word2vec_example.png)\n",
    "\n",
    "These initial word vectors contained a dimensionality of 50–100 values. The encoding mechanism of these vectors meant that similar words would be grouped together (Monday, Tuesday, etc) — and calculations on the vector space could produce genuinely insightful relationships.\n",
    "\n",
    "![Arithmetic using word vectors](../../assets/images/king_man_woman_queen_example.png)\n",
    "\n",
    "A well-known example is that of taking the vector for *King*, subtracting the vector *Man*, and adding the vector *Woman* resulting in the nearest datapoint being *Queen*.\n",
    "\n",
    "## Recurrence\n",
    "\n",
    "During this boom in NLP, the recurrent neural network (RNN) quickly became the favorite for most language applications. RNNs suited language well thanks to their *recurrence*.\n",
    "\n",
    "![Recurrent neural net](../../assets/images/rnn.png)\n",
    "\n",
    "A recurrent neural network unit will consume the first time-step *'the'*, pass on its output state to the next time-step *'quick'* — this recurrent process continues for a specified length of time-steps (the sequence length).\n",
    "\n",
    "This recurrence allowed the neural nets to consider the order of words and their effect on preceding and subsequent words — allow the nuances of human language to be better represented.\n",
    "\n",
    "Although we didn’t see their popular usage until 2013 — the concept and methodology of RNNs were being discussed across several papers in the 80s \\[1\\], \\[2\\].\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "RNNs came with their problems, primarily the vanishing gradient problem. The recurrence of these networks means that they are by nature very deep networks with many points containing an operation between the incoming data and the neuron weight.\n",
    "\n",
    "When calculating the error of the network, and using that to update the network weights, we walk back through the network updating weight after weight.\n",
    "\n",
    "If the update gradient is a small number, we are multiplying an increasingly smaller and smaller number — meaning the full network either takes a very long time to train or simply does not work.\n",
    "\n",
    "On the other hand, if our weight recurring value is too high — we suffer from the exploding gradient problem. Here, the network weights will oscillate without learning any meaningful representation.\n",
    "\n",
    "### Long Short Term Memory\n",
    "\n",
    "The solution to the vanishing gradients problem came with the introduction of long short-term memory (LSTM) units.\n",
    "\n",
    "![Flow of information through a single LSTM unit](../../assets/images/lstm.png)\n",
    "\n",
    "LSTM units introduced a more stable passage of information — the cell state, shown in black above. This additional stream of information was controlled that the chain of time-states with a minimal number of transformations controlled by *'gates'*.\n",
    "\n",
    "![Flow of information over several time-steps](../../assets/images/lstm_cell_state.png)\n",
    "\n",
    "This allowed long-term dependencies to be learned by allowing information from much earlier in a sequence to be retained and applied to states much later in the sequence.\n",
    "\n",
    "### Attention\n",
    "\n",
    "Very quickly recurrent encoder-decoder models were complemented with additional hidden states and neural network layers — these produced the attention mechanism.\n",
    "\n",
    "![Attention mechanism in LSTM network](../../assets/images/lstm_attention.png)\n",
    "\n",
    "Adding encoder-decoder networks allowed the output layers of a model to not only receive the final-state of the RNN units — but to also receive information from every state of the input layer, creating an ‘attention’ mechanism.\n",
    "\n",
    "![Attention between encoder and decoder neurons during English-French translation task](../../assets/images/attention_example.png)\n",
    "\n",
    "Using this approach, we find that similarity between the encoder and decoder states will result in a higher weight — producing outcomes like that of the French translation image above.\n",
    "\n",
    "With this encoder-decoder implementation three tensors, the *Query*, *Key*, and *Value* are used in the attention operation. The *Query* is pulled from the hidden state of the decoder at every time-step — alignment between this and the *Key-Value* tensors is assessed to produce the context vector.\n",
    "\n",
    "![Context vector in LSTM attention network](../../assets/images/context_vector_lstm.png)\n",
    "\n",
    "The context vector is then passed back into the decoder where is used to produce a prediction for that time-step. This process is repeated for every time-step in the decoder space.\n",
    "\n",
    "## Attention Is All You Need\n",
    "\n",
    "Solo-attention began with the 2017 paper *'Attention Is All You Need'* \\[3\\]. You may have guessed it already, this paper introduced the idea that we don’t need to use these complex convolutional or recurrent neural networks alongside attention — attention is in fact, all you need.\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "Self-attention was a key factor for this to function. It meant that whereas before the Query came from the output decoder, it is now generated directly from the input values alongside Key and Value.\n",
    "\n",
    "![Self attention, attention changes with a slight variation in the sentences](../../assets/images/self_attention_two_diff_phrases.png)\n",
    "\n",
    "Because the Query, Key, and Value are all produced by the input, we are able to encode the alignment between different parts of the same input sequence. If we take the image above, we can see that changing the final word from *tired* to *wide* shifts the attention focus from *animal* to *street*.\n",
    "\n",
    "This allows the attention mechanism to encode relationships between all of the words in the input data.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "The next big change to the attention mechanism was the addition of multiple attention heads — essentially many self-attention operations performed in parallel and initialized with different weights.\n",
    "\n",
    "![Multi-head attention](../../assets/images/multi_head_attention.png)\n",
    "\n",
    "Multi-head attention refers to the processing of multiple attention ‘heads’ in parallel. The outputs of these multiple heads are concatenated together.\n",
    "\n",
    "Without multi-head attention, the A. Vaswani et al. transformer model actually performed worse than many of its predecessors \\[3\\].\n",
    "\n",
    "The parallel mechanism allowed the model to represent several *subspaces* of the same sequence. These different levels of attention were then concatenated and processed by a linear unit.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The input to a Transformer model is not sequential like RNNs. In the past, it was this sequential operation allowed us to consider the position and order of words.\n",
    "\n",
    "To maintain the positional information of words a positional encoding is added to the word embedding before entering the attention mechanism.\n",
    "\n",
    "The approach taken in the Attention Is All You Need paper was to produce a different sinusoidal function for every dimension in the embedding dimension.\n",
    "\n",
    "Remember before we said word2vec introduced to the concept of representing a word as many numbers in a 50 to 100-dimensional vector? Here, in the Vaswani et al. paper, they use the same idea but to represent the position of a word.\n",
    "\n",
    "However, this time — rather than calculating the vector values using an ML model, the values are calculated using modified sinusoidal functions:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
    "$$\n",
    "\n",
    "The two formulae are our alternativing positional encoding values. Using the word position **pos**, embedding dimension **i**, and the number of embedding dimensions **d_model**. Together they produce this:\n",
    "\n",
    "![Sine followed by cosine function](../../assets/images/sin_cos_alternating_pos_encoding.png)\n",
    "\n",
    "Each index of the vector is assigned an alternating sine-cosine-sine function (index 0 is sine, index 1 is cosine). Next, as the index value increases away from zero towards d (the embedding dimensionality) the frequency of the sinusoidal function decreases:\n",
    "\n",
    "![Frequency change with increasing embedding indices](../../assets/images/sin_func_embeddings_frequency.png)\n",
    "\n",
    "We can take the same unruly sinusoidal plot from above, add in the 512 embedding dimensionality used in the A. Vaswani et al. paper and map these onto an *easier* to understand heatmap:\n",
    "\n",
    "![Positional encoding heatmap](../../assets/images/positional_encoding_heatmap.png)\n",
    "\n",
    "We can see the higher frequency in the lower embedding dimensions (left) — which decreases as the embedding dimension increases. Around dimension 24, the frequency has decreased so much that we no longer see any change in the remaining (alternating) sine-cosine waves.\n",
    "\n",
    "These positional encodings are then added to the word embeddings.\n",
    "\n",
    "*As a side note, this means that both the word embedding dimensionality and positional encoding dimensionality must match.*\n",
    "\n",
    "### The Transformer\n",
    "\n",
    "![First transformer architecture from A. Vaswani et al. paper](../../assets/images/first_transformer.png)\n",
    "\n",
    "The resultant architecture of these changes to the attention model produced the world’s first Transformer.\n",
    "\n",
    "Beyond the word embedding, positional encodings, and multi-head self-attention operations already discussed — the model is reasonably easy to comprehend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
