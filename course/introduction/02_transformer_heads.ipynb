{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Modules\n",
    "\n",
    "For most transformer models, we are able to modify their behavior for different tasks by adding different *'modules'* (or *heads*) on top of the models. For example, with BERT we could add:\n",
    "\n",
    "* Masked Language Modeling (MLM) head\n",
    "\n",
    "* Question-Answering head\n",
    "\n",
    "* Classification head\n",
    "\n",
    "And so on.\n",
    "\n",
    "## MLM Head\n",
    "\n",
    "For masked language modeling, text will be fed in with a set of tokens being masked with a *\\[MASK\\]* token - which BERT must then guess.\n",
    "\n",
    "To do this, we use a linear layer which takes the *hidden_state* activation from the final BERT layers (which is typically 768 or 1024) values in length, and process then into another vector equal to the length of the *vocabulary*, we then apply a softmax function to this vector to get the probability of each word in the vocabulary. An argmax is taken from this vector.\n",
    "\n",
    "![Masked Language Modeling head for BERT MLM](../../assets/images/bert_mlm.png)\n",
    "\n",
    "## Classification Head\n",
    "\n",
    "Classification heads follow a very similar setup to the MLM heads, the only difference being that the linear layer maps to a smaller vector where each value corresponds to a *class* in the output.\n",
    "\n",
    "It's worth noting that this is the simplest approach, we could build out a classification head using LSTM layers and several linear layers - but this doesn't tend to improve results.\n",
    "\n",
    "## Question-Answering Head\n",
    "\n",
    "For question-answering we will be feeding in a *question*, alongside the *context* that contains our *answer*. From this, we would like to output a *start token* and *end token* that marks the start-end positions of our *answer* from within the *context*.\n",
    "\n",
    "There are different architectures to achieve this, but the simplests consists of a *start token classifier*, and an *end token classifier*, both are built of a single linear layer and softmax activation function - the start and end positions are then given by the argmax of the output layer probabilities, all of this looks like:\n",
    "\n",
    "![Q&A BERT showing additional start/end token classifiers](../../assets/images/qa_linear_bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
