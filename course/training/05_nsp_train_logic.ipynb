{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSP Training Logic\n",
    "\n",
    "Next sentence prediction (NSP) is the other side of pretraining for BERT. It consists of taking two sentences, A and B - and attempting to guess (classification) whether sentence B comes after sentence A.\n",
    "\n",
    "So, where MLM allowed us to encourage BERT to build up a contextual understanding between words - NSP encourages BERT to learn longer term contextual relationships between sentences rather than words.\n",
    "\n",
    "Let's take a look at how this works in code. First, we import and initialize everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = (\"After Abraham Lincoln won the November 1860 presidential election on an \"\n",
    "        \"anti-slavery platform, an initial seven slave states declared their \"\n",
    "        \"secession from the country to form the Confederacy. War broke out in \"\n",
    "        \"April 1861 when secessionist forces attacked Fort Sumter in South \"\n",
    "        \"Carolina, just over a month after Lincoln's inauguration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we were to tokenize and process this text through our model as is, we'll the *logits* tensor as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *logits* tensor is our NSP output prediction, which looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.4646, -3.6635]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply softmax to convert these logits into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9970e-01, 2.9502e-04]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, take the argmax to get our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting **0**, which is `IsNextSentence` - however, we haven't actually specified two sentences - so this prediction is meaningless.\n",
    "\n",
    "There are two parts we're missing. We need to specify two sentences in our *input_ids* tensor - and we need to create the *labels* tensor too.\n",
    "\n",
    "Let's start by splitting the two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"After Abraham Lincoln won the November 1860 presidential election on an \"\n",
    "        \"anti-slavery platform, an initial seven slave states declared their \"\n",
    "        \"secession from the country to form the Confederacy.\")\n",
    "text2 = (\"War broke out in April 1861 when secessionist forces attacked Fort \"\n",
    "         \"Sumter in South Carolina, just over a month after Lincoln's \"\n",
    "         \"inauguration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, text2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2602,\n",
       "          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
       "          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,\n",
       "          1996, 18179,  1012,   102,  2162,  3631,  2041,  1999,  2258,  6863,\n",
       "          2043, 22965,  2923,  2749,  4457,  3481,  7680,  3334,  1999,  2148,\n",
       "          3792,  1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055,\n",
       "         17331,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we tokenized the two sentences seperately, our tokenizer will deal it a little differently. First, we can see the *SEP* token in the *input_ids* tensor with token id **102** - this marks the boundary between sentence A and sentence B.\n",
    "\n",
    "Second, we can distinguish between sentences from the *token_type_ids* - sentence A tokens are assigned a **0**, whereas sentence B tokens are assigned a **1** token.\n",
    "\n",
    "Now, we still need to add our *labels* tensor - but how should it be formatted? Well, we use a `torch.LongTensor` format, and it must contain a single value `[0]` if sentence B **is** the next sentence, else it should be `[1]`. Here, sentence B is the next sentence so we set it to `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.LongTensor([0])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we process everything as we did before, including our *labels* tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs, labels=labels)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we return the *loss* tensor - which we use for training our model with NSP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
