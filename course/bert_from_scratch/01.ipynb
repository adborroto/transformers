{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ml_test",
   "display_name": "ML test",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "First we will initialize our filiBERTo tokenizer like before, and use it to encode our data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "tokenizer = RobertaTokenizer.from_pretrained('filiberto', max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm(tensor):\n",
    "    # create random array of floats with equal dims to tensor\n",
    "    rand = torch.rand(tensor.shape)\n",
    "    # mask random 15% where token is not 0 <s>, 1 <pad>, or 2 <s/>\n",
    "    mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "    # loop through each row in tensor (cannot do in parallel)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        # get indices of mask positions from mask array\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        # mask tensor\n",
    "        tensor[i, selection] = 4\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 200/200 [5:16:30<00:00, 94.95s/it]\n"
     ]
    }
   ],
   "source": [
    "paths = [str(x) for x in Path('../../data/text/oscar_it').glob('**/*.txt')]\n",
    "# initialize lists of tensors\n",
    "input_ids = []\n",
    "mask = []\n",
    "labels = []\n",
    "# open all files, encode and add to single dataset\n",
    "for path in tqdm(paths[:200]):\n",
    "    # :50\n",
    "    # open the file and split into list by newline characters\n",
    "    with open(path, 'r', encoding='utf-8') as fp:\n",
    "        lines = fp.read().split('\\n')\n",
    "    # encode\n",
    "    sample = tokenizer(lines, max_length=512, truncation=True, padding='max_length')\n",
    "    # convert tokens to tensor\n",
    "    labels.append(torch.tensor(sample.input_ids))\n",
    "    # create attention mask tensor\n",
    "    mask.append(torch.tensor(sample.attention_mask))\n",
    "    # mask ~15% of tokens to create inputs\n",
    "    input_ids.append(mlm(labels[-1].detach().clone()))\n",
    "# convert lists of tensors into tensors\n",
    "input_ids = torch.cat(input_ids)\n",
    "mask = torch.cat(mask)\n",
    "labels = torch.cat(labels)"
   ]
  },
  {
   "source": [
    "We have 2000000 tokenized sequences, each containing 512 tokens:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2000000, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([    0,   696, 17974,  1298,  7479,   292,  1081,     4, 11222,  7427,\n",
       "          787,   742,  2014,   280,    11, 10012,  3686,  1314,  2192,  1616,\n",
       "          605,  1168, 14686,    30,   567,   267,  1320,    16,   383,  3439,\n",
       "          458, 10418,  9303,   375, 23989,  2982,  1185,   404,  2506,   425,\n",
       "        26047, 20159,   722,   304,   306, 22536,   375,  8832, 19775,   964,\n",
       "          439,     4,  1466,   275,  2473,   404,    37,     4,     4,   722,\n",
       "        15001,   272,   275,     4,   625,   771, 18443,   481,     4,   275,\n",
       "        11936,   536,    18,   683,  3283, 22100,   375,     4, 15637,     4,\n",
       "         1122,     4,   269,   427,   286, 12010,   481,  8605,  6033,   275,\n",
       "         4858,    18, 11510,   368,   275,  6231,   315,     4,   582,  2215,\n",
       "        12673,    18,     4,     4,   369,   451,  2147,     4,     4,   490,\n",
       "          315,  6907,  6887,     4,   582,  2215, 12673,     4, 14091,     4,\n",
       "          792, 10761,  6561,  2556,  5929,     4,   460,     4,    18,   683,\n",
       "         1065,  1105,  1810, 22179, 10395,   305,  3819,   306,  6080,   292,\n",
       "        17151,    18,   388,   324,  4214,   341,  4491,   313,     4, 17661,\n",
       "            4,  5066,   315,     4,  1105,  1810,    16,  5417,   337,   324,\n",
       "         3394,   439,     4,  4914,   507,     4,    12,    39,    13,     4,\n",
       "        17924,     4,    17,  6001,  1081,   939,   363,    18,     4,    94,\n",
       "         1557,   591,  1727,     4,   341,     4, 17802,   298,  1105,  4984,\n",
       "           16,  7656, 10874,     4,  2324,  3630,   425,     4, 20159,    18,\n",
       "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "source": [
    "We can see the special tokens here, `0` is our **<s\\>** token, `2` our **<s\\\\>** token, `3` our **<mask\\>** token, and at the end we have two `1` - or **<pad\\>** - tokens.\n",
    "\n",
    "Let's save these tensors to file for if we need to do any further training later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./filiberto_training'):\n",
    "    os.mkdir('./filiberto_training')\n",
    "\n",
    "torch.save(input_ids, './filiberto_training/input_ids.pt')\n",
    "torch.save(mask, './filiberto_training/attention_mask.pt')\n",
    "torch.save(labels, './filiberto_training/labels.pt')\n",
    "\n",
    "del input_ids, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.load('./filiberto_training/input_ids.pt')\n",
    "mask = torch.load('./filiberto_training/attention_mask.pt')\n",
    "labels = torch.load('./filiberto_training/labels.pt')"
   ]
  },
  {
   "source": [
    "Now let's move onto building our dataset and dataloader."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "source": [
    "Next we initialize our `Dataset`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "source": [
    "And initialize the dataloader, which will load the data into the model during training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "source": [
    "And move onto building our model, we first need to create a RoBERTa config object, which will describe which features we want to initialize our RoBERTa model with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab set in previous notebook\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "source": [
    "Then we import and initialize a RoBERTa model with a language modeling head."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "source": [
    "And now we move onto training. First we setup GPU/CPU usage."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, if using tensorboard, initialize writer object\n",
    "writer = torch.utils.tensorboard.SummaryWriter()"
   ]
  },
  {
   "source": [
    "Now we move onto the training loop."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 0: 100%|██████████| 125000/125000 [17:26:15<00:00,  1.99it/s, loss=0.489]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "epochs = 1  # trained for 4 in total\n",
    "step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # take loss for tensorboard\n",
    "        writer.add_scalar('Loss/train', loss, step)\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./filiberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}