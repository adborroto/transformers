{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity and Distance Metrics\n",
    "\n",
    "In this notebook we'll cover the three key similarity and distance metrics used in NLP, *Euclidean distance*, *cosine similarity*, and *dot product similarity*.\n",
    "\n",
    "First, let's define three vectors - `a`, `b`, and `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.01, 0.07, 0.1]\n",
    "b = [0.01, 0.08, 0.11]\n",
    "c = [0.91, 0.57, 0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance\n",
    "\n",
    "Euclidean distance is the simplest *similarity* metric - it is the only metric to measure *distance* between two points. We also call this the **L2 norm** metric. Given two vectors **u** and **v** it is calculated using:\n",
    "\n",
    "$$\n",
    "d(u, v) = \\sqrt{\\sum_{i=1}^{n}(u_i - v_i)^2}\n",
    "$$\n",
    "\n",
    "So for our vectors **a** and **b** this would look like:\n",
    "\n",
    "$$\n",
    "d(a, b) = \\sqrt{(b_1 - a_1)^2 + (b_2 - a_2)^2 + (b_3 - a_3)^2} = \\sqrt{(0.01 - 0.01)^2 + (0.08 - 0.07)^2 + (0.11 - 0.1)^2} = 0.0141\n",
    "$$\n",
    "\n",
    "In Python (using Numpy) we would calculate the Euclidean distance like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014142135623730944"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sqrt(sum(np.square(np.subtract(a, b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that our approach is correct by using the `scipy.spatial` `distance` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1358697108383513"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sum(np.square(np.subtract(b, c))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014142135623730944"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "distance.euclidean(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "The dot product considers both direction, and magnitude. It is calculated as:\n",
    "\n",
    "$$\n",
    "u \\cdot v = \\vert u \\vert \\vert v \\vert cos \\theta = \\sum_{i=1}^{n}a_n b_n\n",
    "$$\n",
    "\n",
    "For our vectors **a** and **b**:\n",
    "\n",
    "$$\n",
    "a \\cdot b = (a_1 b_1) + (a_2 b_2) + (a_3 b_3) = (0.01 * 0.01) + (0.07 * 0.08) + (0.1 * 0.11) = 0.0167\n",
    "$$\n",
    "\n",
    "We calculate the dot product easily with Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0167"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Which is the same as `np.matmul` when transposing one of the vectors - `np.dot` performs this transpose operation automatically)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0167"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a, np.array(b).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is written in plain Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016700000000000003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]*b[0] + a[1]*b[1] + a[2]*b[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only drawback of using dot product is that it is not normalized by scale, so larger vectors will tend to score higher dot products, even if they are less similiar. For example, vectors `a` and `c` are exactly similar to themselves - but dot product sees `c` as being more similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015000000000000003"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10899999999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.513"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(c, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so we must find a way to normalize..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity is through-and-through a *similarity* metric. This is because, if two vectors are oriented in the same direction, the angle between them will be *very large* - meaning the cosine similarity will be *very small* (eg they are not similar).\n",
    "\n",
    "We calculate it like so:\n",
    "\n",
    "$$\n",
    "sim(u, v) = \\frac{u \\cdot v}{\\lVert u \\rVert \\lVert v \\rVert} = \\frac{\\sum_{i=1}^{n}a_n b_n}{\\sqrt{\\sum_{i=1}^{n}u_{n}^2}\\sqrt{\\sum_{i=1}^{n}v_{n}^2}}\n",
    "$$\n",
    "\n",
    "The cosine similarity calculation takes the dot product between two vectors (which considers both magnitude and direction), and divides it by the cross product of both vectors (the length of both, multiplied together). This process means that we calculate the `(magnitude and direction) / magnitude` - leaving us with just the direction - eg the angular/directional similarity.\n",
    "\n",
    "So this metric is like a *normalized* dot product!\n",
    "\n",
    "We can apply to to our vectors **a** and **b**:\n",
    "\n",
    "$$\n",
    "sim(a, b) = \\frac{(a_1 * b_1) + (a_2 * b_2) + (a_3 * b_3)}{\\sqrt{a_{1}^2+a_{2}^2+a_{3}^2}\\sqrt{b_{1}^2+b_{2}^2+b_{3}^2}} = \\frac{(0.01 * 0.01) + (0.07 * 0.08) + (0.1 * 0.11)}{\\sqrt{0.01^2+0.07^2+0.1^2}\\sqrt{0.01^2+0.08^2+0.11^2}} = \\frac{0.0167}{0.016703} = 0.9998\n",
    "$$\n",
    "\n",
    "And in Python with Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998028479490471"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, b) / (np.sqrt(sum(np.square(a))) * np.sqrt(sum(np.square(b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can confirm this using another implementation, this time from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99980285]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([a], [b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this two two of the same vector (eg exactly similar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([a], [a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will get the exact same value for `c`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([c], [c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([a], [a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7235381]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([a], [c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it seems that *cosine similarity* is the metric to use at all times? Well, no. We will still often use *dot product* similarity because it is less computationally expensive (important for large datasets). As with cosine similarity we compute the dot product, and then normalize - which increases calculation complexity.\n",
    "\n",
    "Here's a little walkthrough of dot product and cosine similarity calculations for our three vectors:\n",
    "\n",
    "![Dot product and cosine similarity](../../assets/images/dot_product_and_cosine_similarity_workthrough.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
