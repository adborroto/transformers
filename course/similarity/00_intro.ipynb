{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity and Transformers\n",
    "\n",
    "A big part of NLP relies on measuring similarity in highly-dimensional spaces. Many NLP solutions take some text, process it to create a big vector/array representing said text - then perform several transformations.\n",
    "\n",
    "In this section we'll cover measuring sentence similarity - a great example of how powerful simple similarity metrics can be when paired with transformers.\n",
    "\n",
    "The logic in producing a solution like this can be split into two steps:\n",
    "\n",
    "* Take sentences, convert them into vectors (using a transformer).\n",
    "\n",
    "* Use a similarity metric (more on this later) to find the closest matching sentences.\n",
    "\n",
    "## Dense Vectors\n",
    "\n",
    "BERT and many other transformer models are able to take text, and encode a significant amount of information into vectors which represent that text. This means that semantically similar sentences, will be represented by similar vectors.\n",
    "\n",
    "We call these vectors *dense* because every value within the vector has a value and has a reason for being that value - this is in contrast to sparse vectors, such as one-hot encoded vectors where the majority of values are 0. BERT is great at creating these dense vectors, and each encoder layer (there are several) outputs a set of dense vectors.\n",
    "\n",
    "![BERT base network with hidden layer representations highlighted in green](../../assets/images/bert_embeddings.png)\n",
    "\n",
    "For BERT base, this will be a vector containing 768. Those 768 values contain our numerical representation of a single token - which we can use as contextual word embeddings.\n",
    "\n",
    "Because there is one of these vectors for representing each token (output by each encoder), we are actually looking at a tensor of size 768 by the number of tokens. We can take these tensors - and transform them to create semantic representations of the input sequence. We can then take our similarity metrics and calculate the respective similarity between different sequences.\n",
    "\n",
    "The simplest and most commonly extracted tensor is the `last_hidden_state` tensor - which is conveniently output by the BERT model. Of course, this is a pretty large tensor - at 512x768 - and we want a vector to apply our similarity measures to it. To do this, we need to convert our `last_hidden_states` tensor to a vector of 768 dimensions.\n",
    "\n",
    "This compressed vector is our **sentence embedding** (or sentence *vector*) - it is this that we then take and compare to other *sentence embeddings* using a similarity metric (like cosine similarity - again, more on this later).\n",
    "\n",
    "After that, we're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
