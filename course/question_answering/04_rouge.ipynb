{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE\n",
    "\n",
    "**ROUGE** stands for **R**ecall-**O**riented **U**nderstudy for **G**isting **E**valuation. The name is deceptively complicated, because this is not a difficult metric to understand, and it's incredibly easy to implement.\n",
    "\n",
    "## What is ROUGE\n",
    "\n",
    "ROUGE is actually a set of metrics, rather than just one. We will cover the main ones that are most likely to be used, starting with ROUGE-N.\n",
    "\n",
    "### ROUGE-N\n",
    "ROUGE-N measures the number of matching 'n-grams' between our model predicted answer and a *'reference'*.\n",
    "\n",
    "An n-gram is simply a grouping of tokens/words. A unigram (1-gram) would consist of a single word. A bigram (2-gram) consists of two consecutive words:\n",
    "\n",
    "Original: `\"the quick brown fox jumps over\"`\n",
    "\n",
    "**Uni**grams: `['the', 'quick', 'brown', 'fox', 'jumps', 'over']`\n",
    "\n",
    "**Bi**grams: `['the quick', 'quick brown', 'brown fox', 'fox jumps', 'jumps over']`\n",
    "\n",
    "**Tri**grams: `['the quick brown', 'quick brown fox', 'brown fox jumps', 'fox jumps over']`\n",
    "\n",
    "The *reference* in our case is our true answer.\n",
    "\n",
    "With ROUGE-N, the N represents the n-gram that we are using. For ROUGE-1 we would be measuring the match-rate of unigrams between our model output and reference.\n",
    "\n",
    "ROUGE-2 and ROUGE-3 would use bigrams and trigrams respectively.\n",
    "\n",
    "Once we have decided which N to use — we now decide on whether we’d like to calculate the ROUGE recall, precision, or F1 score.\n",
    "\n",
    "### Recall\n",
    "The recall counts the number of overlapping n-grams found in both the model output and reference — then divides this number by the total number of n-grams in the reference. It looks like this:\n",
    "\n",
    "![ROUGE-N recall calculation](../../assets/images/rouge_recall.png)\n",
    "\n",
    "This is great for ensuring our model is **capturing all of the information** contained in the reference — but this isn’t so great at ensuring our model isn’t just pushing out a huge number of words to game the recall score:\n",
    "\n",
    "![ROUGE-N recall can be gamed easily](../../assets/images/rouge_gaming_recall.png)\n",
    "\n",
    "### Precision\n",
    "\n",
    "To avoid this we use the precision metric — which is calculated in almost the exact same way, but rather than dividing by the reference n-gram count, we divide by the model n-gram count.\n",
    "\n",
    "![ROUGE-N precision calculation](../../assets/images/rouge_precision_calc.png)\n",
    "\n",
    "So if we apply this to our previous example, we get a precision score of just 43%:\n",
    "\n",
    "![ROUGE-N precision scores predictions that could trick recall poorly](../../assets/images/rouge_precision_fixes_recall.png)\n",
    "\n",
    "### F1-Score\n",
    "\n",
    "Now that we both the recall and precision values, we can use them to calculate our ROUGE F1 score like so:\n",
    "\n",
    "![F1 score calculation](../../assets/images/rouge_f1_calc.png)\n",
    "\n",
    "Let's apply that again to our previous example:\n",
    "\n",
    "![F1 score on example](../../assets/images/rouge_f1.png)\n",
    "\n",
    "That gives us a reliable measure of our model performance that relies not only on the model capturing as many words as possible (recall) but doing so without outputting irrelevant words (precision).\n",
    "\n",
    "### ROUGE-L\n",
    "\n",
    "ROUGE-L measures the **longest common subsequence (LCS)** between our model output and reference. All this means is that we count the longest sequence of tokens that is shared between both:\n",
    "\n",
    "![ROUGE-L finds the longest common subsequence](../../assets/images/rouge_l.png)\n",
    "\n",
    "The idea here is that a longer shared sequence would indicate more similarity between the two sequences. We can apply our recall and precision calculations just like before — but this time we replace the match with LCS.\n",
    "\n",
    "First we calculate the LCS recall:\n",
    "\n",
    "![ROUGE-L recall](../../assets/images/rouge_l_recall.png)\n",
    "\n",
    "Precision is the same, we just switch our total n-gram count from the reference to the model:\n",
    "\n",
    "![ROUGE-L precision](../../assets/images/rouge_l_precision.png)\n",
    "\n",
    "And finally, we calculate the F1 score just like we did before:\n",
    "\n",
    "![ROUGE-L F1](../../assets/images/rouge_l_f1.png)\n",
    "\n",
    "### ROUGE-S\n",
    "\n",
    "The final ROUGE metric we will look at is the ROUGE-S — or skip-gram concurrence metric.\n",
    "\n",
    "Now, this metric is much less popular than ROUGE-N and ROUGE-L covered already — but it’s worth being aware of what it does.\n",
    "\n",
    "Using the skip-gram metric allows us to search for consecutive words from the reference text, that appear in the model output but are separated by one-or-more other words.\n",
    "\n",
    "So, if we took the bigram “the fox”, our original ROUGE-2 metric would only match this if this exact sequence was found in the model output. If the model instead outputs “the brown fox” — no match would be found.\n",
    "\n",
    "ROUGE-S allows us to add a degree of leniency to our n-gram matching. For our bigram example we could match by using a skip-bigram measure:\n",
    "\n",
    "![ROUGE-S recall](../../assets/images/rouge_s_recall.png)\n",
    "\n",
    "The same logic applies to our precision metric too:\n",
    "\n",
    "![ROUGE-S precision](../../assets/images/rouge_s_precision.png)\n",
    "\n",
    "After calculating our recall and precision, we can calculate the F1 score too just as we did before.\n",
    "\n",
    "### Cons\n",
    "\n",
    "ROUGE is a great evaluation metric but comes with some drawbacks. In-particular, ROUGE does not cater for different words that have the same meaning — as it measures syntactical matches rather than semantics.\n",
    "\n",
    "So, if we had two sequences that had the same meaning — but used different words to express that meaning — they could be assigned a low ROUGE score.\n",
    "\n",
    "This can be offset slightly by using several references and taking the average score, but this will not solve the problem entirely.\n",
    "\n",
    "Nonetheless, it’s a good metric which is very popular for assessing the performance of several NLP tasks, including machine translation, automatic summarization, and *for us*, question-and-answering.\n",
    "\n",
    "## In Python\n",
    "\n",
    "We've worked through the theory of the ROUGE metrics and how they work. Fortunately, implementing these metrics in Python is incredibly easy thanks to the Python rouge library.\n",
    "\n",
    "We can install the library through pip:\n",
    "\n",
    "```\n",
    "pip install rouge\n",
    "```\n",
    "\n",
    "And scoring our model output against a reference is as easy as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.6666666622222223, 'p': 0.5, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.6666666622222223, 'p': 0.5, 'r': 1.0}}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "model_out = 'hello to the world'\n",
    "reference = 'hello world'\n",
    "\n",
    "# initialize the rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# get the scores\n",
    "rouge.get_scores(model_out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_scores` method returns three metrics, ROUGE-N using a unigram (ROUGE-1) and a bigram (ROUGE-2) — and ROUGE-L.\n",
    "\n",
    "For each of these, we receive the F1 score **f**, precision **p**, and recall **r**.\n",
    "\n",
    "Let's apply this to our set of five answers and see what we get. First, we need to define the `answers` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [{'predicted': 'France', 'true': 'France.'},\n",
    "           {'predicted': 'in the 10th and 11th centuries',\n",
    "            'true': '10th and 11th centuries'},\n",
    "           {'predicted': '10th and 11th centuries', 'true': '10th and 11th centuries'},\n",
    "           {'predicted': 'Denmark, Iceland and Norway',\n",
    "            'true': 'Denmark, Iceland and Norway'},\n",
    "           {'predicted': 'Rollo', 'true': 'Rollo,'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to reformat this list into two lists, one for our predictions `model_out` and another for the true answers `reference`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = [ans['predicted'] for ans in answers]\n",
    "\n",
    "reference = [ans['true'] for ans in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['France',\n",
       " 'in the 10th and 11th centuries',\n",
       " '10th and 11th centuries',\n",
       " 'Denmark, Iceland and Norway',\n",
       " 'Rollo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass both of these lists to the `rouge.get_scores` method to return a list of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.999999995, 'p': 1.0, 'r': 1.0}},\n",
       " {'rouge-1': {'f': 0.7999999952000001, 'p': 0.6666666666666666, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.7499999953125, 'p': 0.6, 'r': 1.0},\n",
       "  'rouge-l': {'f': 0.7999999952000001, 'p': 0.6666666666666666, 'r': 1.0}},\n",
       " {'rouge-1': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-l': {'f': 0.999999995, 'p': 1.0, 'r': 1.0}},\n",
       " {'rouge-1': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-l': {'f': 0.999999995, 'p': 1.0, 'r': 1.0}},\n",
       " {'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(model_out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we want to get average metrics for all answers, we can do this by adding `avg=True` to the `get_scores` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.7599999960400001, 'p': 0.7333333333333333, 'r': 0.8},\n",
       " 'rouge-2': {'f': 0.5499999970625, 'p': 0.52, 'r': 0.6},\n",
       " 'rouge-l': {'f': 0.7599999960400001, 'p': 0.7333333333333333, 'r': 0.8}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we've explored a few more insightful metrics for measuring our Q&A model performance. Going forwards, we'll be using **ROUGE** a lot, so it's good to get familiar with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
