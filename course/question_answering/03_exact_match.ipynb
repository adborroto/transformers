{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Match Metric\n",
    "\n",
    "The exact match (EM) metric does what you would expect it to. It returns a boolean value, yes or no, as to whether our predicted text matches to our true text. Let's take the following answers from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [{'predicted': 'France', 'true': 'France.'},\n",
    "           {'predicted': 'in the 10th and 11th centuries',\n",
    "            'true': '10th and 11th centuries'},\n",
    "           {'predicted': '10th and 11th centuries', 'true': '10th and 11th centuries'},\n",
    "           {'predicted': 'Denmark, Iceland and Norway',\n",
    "            'true': 'Denmark, Iceland and Norway'},\n",
    "           {'predicted': 'Rollo', 'true': 'Rollo,'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the EM accuracy of our model using these five predictions, all we need to do is iterate through each prediction, and append a `1` where there is an exact match, or a `0` where there is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = []\n",
    "\n",
    "for answer in answers:\n",
    "    if answer['predicted'] == answer['true']:\n",
    "        em.append(1)\n",
    "    else:\n",
    "        em.append(0)\n",
    "\n",
    "# then total up all values in em and divide by number of values\n",
    "sum(em)/len(em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 40% EM score, which doesn't look very good despite the fact that we got incredibly close on every single answer. This is one of the limitations of using the EM metric, but we can make it slightly more lenient. For example our first answer returns *`'France'`* and *`'France.'`*, the only difference being the final punctuation which is included in the *true* answer (which is actually less correct that what our model predicted).\n",
    "\n",
    "We can clean each side of our text before comparison to remove these minor differences and return an exact match. For this, we can use regular expressions. We will remove any character which is not a space, letter, or number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "em = []\n",
    "\n",
    "for answer in answers:\n",
    "    pred = re.sub('[^0-9a-z ]', '', answer['predicted'].lower())\n",
    "    true = re.sub('[^0-9a-z ]', '', answer['true'].lower())\n",
    "    if pred == true:\n",
    "        em.append(1)\n",
    "    else:\n",
    "        em.append(0)\n",
    "\n",
    "# then total up all values in em and divide by number of values\n",
    "sum(em)/len(em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a slightly better score of 80%, but this is still not representative of the models performance. Ideally, we want to be turning to more advanced metrics that can deal with more *fuzzy* logic. We will be covering one of those methods next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
