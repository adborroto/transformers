{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Unicode normalization is used to *normalize* different but similiar characters. For example the following unicode characters (and character combinations) are equivalent:\n",
    "\n",
    "**Canonical Equivalence**\n",
    "\n",
    "| | | Equivalence Reason |\n",
    "| --- | --- | --- |\n",
    "| Ç | C◌̧ | Combined character sequences |\n",
    "| 가 | ᄀ ᅡ | Conjoined Korean characters |\n",
    "\n",
    "**Compatibility equivalence**\n",
    "\n",
    "| | | Equivalence Reason |\n",
    "| --- | --- | --- |\n",
    "| ℌ | H | Font variant |\n",
    "| \\[NBSP\\] | \\[SPACE\\] | Both are linebreak sequences |\n",
    "| ① | 1 | Circled variant |\n",
    "| x² | x2 | Superscript |\n",
    "| xⱼ | xj | Subscript |\n",
    "| ½ | 1/2 | Fractions |\n",
    "\n",
    "We have mentioned two different types of equivalence here, canonical and compatibility equivalence.\n",
    "\n",
    "**Canonical equivalence** means both forms are fundamentally the same and when rendered are indistinguishable. For example we can take the unicode for `'Ç' \\u00C7` or the unicode for `'C' \\u0043` and `'̧' \\u0327`, when the latter two characters are rendered together they look the same as the first character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç Ç\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u00C7\", \"\\u0043\"+\"\\u0327\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we print these characters seperately, we can see very clearly that they are not the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç C ̧\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u00C7\", \"\\u0043\", \"\\u0327\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are examples of canonical equivalence, but we also have compatibility equivalence.\n",
    "\n",
    "**Compatibility equivalence** refers to the formatting differences between characters, which includes (but is not limited to):\n",
    "\n",
    "* font variants\n",
    "* cursive forms\n",
    "* circled characters\n",
    "* width variation\n",
    "* size changes\n",
    "* rotation\n",
    "* superscript and subscript\n",
    "* fractions\n",
    "\n",
    "In this case we can see a difference between the rendered characters, for example between `ℌ` and `H`, or `½` and `1 ⁄ 2`.\n",
    "\n",
    "For many of these examples which are either canonical equivalents (Ç ↔ C ̧ ) or compatibility equivalents (½ → 1 ⁄ 2), if we compare if these different forms are equal, we will find that they are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ç\" == \"Ç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ℌ\" == \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"½\" == \"1⁄2\"  # note that 1⁄2 are the characters 1 ⁄ 2 placed together (they are automatically formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is in these cases that we use unicode normalization to *normalize* our characters into matching pairs. As there are different forms of equivalence, there are also different forms of normalization. These are all called **N**ormal **F**orm, and there are four different methods:\n",
    "\n",
    "| Name | Abbreviation | Description | Example |\n",
    "| --- | --- | --- | --- |\n",
    "| Form D | NFD | *Canonical* decomposition | `Ç` → `C ̧` |\n",
    "| Form C | NFC | *Canoncial* decomposition followed by *canonical* composition | `Ç` → `C ̧` → `Ç` |\n",
    "| Form KD | NFKD | *Compatibility* decomposition | `ℌ ̧` → `H ̧` |\n",
    "| Form KC | NFKC | *Compatibility* decomposition followed by *canonical* composition | `ℌ ̧` → `H ̧` → `Ḩ` |\n",
    "\n",
    "Let's take a look at each of these forms in action. Our C with cedilla character Ç can be represented in two ways, as a single character called *Latin capital C with cedilla* (*\\u00C7*), or as two characters called *Latin capital C* (*\\u0043*) and *combining cedilla* (*\\u0327*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ç'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_with_cedilla = \"\\u00C7\"  # Latin capital C with cedilla (single character)\n",
    "c_with_cedilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ç'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_plus_cedilla = \"\\u0043\\u0327\"  # \\u0043 = Latin capital C, \\u0327 = 'combining cedilla' (two characters)\n",
    "c_plus_cedilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will find that these two version do not match when compared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_with_cedilla == c_plus_cedilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform **NFD** on our C with cedilla character `\\u00C7`, we **decompose** the character into it's smaller components, which are the *Latin capital C* character, and *combining cedilla* character `\\u0043` + `\\u0327`. This means that if we compare an **NFD** normalized C with cedilla character to both the C character and the cedilla character, we will return true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFD', c_with_cedilla) == c_plus_cedilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we perform **NFC** on our C with cedilla character `\\u00C7`, we **decompose** the character into the smaller components `\\u0043` + `\\u0327`, and then **compose** them back to `\\u00C7`, and so they will not match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFC', c_with_cedilla) == c_plus_cedilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we switch the **NFC** encoding to instead be performed on our two characters `\\u0043` + `\\u0327`, they will first be **decomposed** (which will do nothing as they are already decomposed), then compose them into the single `\\u00C7` character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_with_cedilla == unicodedata.normalize('NFC', c_plus_cedilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **NFK** encodings do not decompose characters into smaller components, they decompose characters into their *normal* versions. For example if we take the fancy format *ℌ* `\\u210B`, we cannot decompose this into multiple characters and so *NFD* or *NFC* encoding will do nothing. However, if we apply **NFKD**, we will find that our fancy *ℌ* `\\u210B` becomes a plain, boring *H* `\\u0048`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKD', 'ℌ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if we merge our fancy ℌ `\\u210B` with a combining cedilla `\\u0328` character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ℋ̧'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\u210B\\u0327\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying our compatibility decomposition normalization (*NFKD*) gives us a capital H character, and a combining cedilla character as two seperate encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'H\\xcc\\xa7'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKD', \"\\u210B\\u0327\").encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we apply **NFKC**, we first perform compatibility decomposition, into the two seperate characters, before merging them during *canonical* composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe1\\xb8\\xa8'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKC', \"\\u210B\\u0327\").encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the only difference between these two methods is a canonical composition, we see no difference between the two character sets when they are rendered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ḩ', 'Ḩ')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKC', \"\\u210B\\u0327\"), unicodedata.normalize('NFKD', \"\\u210B\\u0327\"), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's it for unicode normalization. When it comes down to it, all you really need to remember is the table we saw above, all of this stuff is quite abstract and not the easiest thing to grasp or remember. So if it seems confusing, that's normal, you'll get by with our normal forms table.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "[UAX #15: Unicode Normalization Forms](https://unicode.org/reports/tr15/)\n",
    "\n",
    "[UTR #15: Unicode Normalization](https://unicode.org/reports/tr15/tr15-18.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
